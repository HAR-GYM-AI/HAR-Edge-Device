{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3e95b9",
   "metadata": {},
   "source": [
    "This is our window rep classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e3c6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "     REP COUNTER - TRAINING PIPELINE (V14 - FINAL TUNED)\n",
      "======================================================================\n",
      "\n",
      "[1/3] Loading and processing data...\n",
      "✓ Loaded 75 sessions.\n",
      "✓ Created 1663 long windows.\n",
      "\n",
      "[2/3] Pivoting data to wide format...\n",
      "✓ Created 1649 unified windows.\n",
      "\n",
      "[3/3] Preparing data and training models...\n",
      "\n",
      "--- Training Stage 1: Activity Detector (0 vs >0) ---\n",
      "Training on 366 'No-Rep' and 122 'Rep' windows (Ratio ~3.0:1).\n",
      "✓ Stage 1 model trained.\n",
      "\n",
      "--- Training Stage 2: Rep Counter (1 vs 2+) ---\n",
      "Training on 122 positive windows. Scale Pos Weight for '2+' class: 0.82\n",
      "✓ Stage 2 model trained.\n",
      "\n",
      "--- Evaluating Final Hierarchical Model with Tuned Threshold ---\n",
      "Using a custom detection threshold of 0.40\n",
      "\n",
      "======================================================================\n",
      "         MODEL PERFORMANCE (THRESHOLD TUNED)\n",
      "======================================================================\n",
      "--- 1. Repetition Volume Analysis (Predicting 0, 1, 2+) ---\n",
      "Total True Reps in Test Set:      67\n",
      "Total Predicted Reps (approx):    65\n",
      "-> Model predicted 97.01% of the actual rep volume.\n",
      "\n",
      "--- 2. Session-Level Performance ---\n",
      "Average absolute error per session: 2.25 reps\n",
      "\n",
      "--- 3. Window-Level Classification (Binned Classes: 0, 1, 2+) ---\n",
      "\n",
      "Confusion Matrix:\n",
      "[[267  24  13]\n",
      " [ 11   6   1]\n",
      " [ 13   5   1]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       304\n",
      "           1       0.17      0.33      0.23        18\n",
      "           2       0.07      0.05      0.06        19\n",
      "\n",
      "    accuracy                           0.80       341\n",
      "   macro avg       0.39      0.42      0.39       341\n",
      "weighted avg       0.83      0.80      0.82       341\n",
      "\n",
      "======================================================================\n",
      "\n",
      "✓ Final hierarchical models and features saved successfully.\n",
      "\n",
      "Pipeline finished.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "from scipy.spatial.transform import Rotation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def detect_reps_from_quaternions(samples, sampling_rate=100):\n",
    "    \"\"\"Analyzes quaternion data to detect repetition peaks.\"\"\"\n",
    "    if len(samples) < 50: return 0\n",
    "    try:\n",
    "        quaternions = np.array([[s['qw'], s['qx'], s['qy'], s['qz']] for s in samples])\n",
    "        rotations = Rotation.from_quat(quaternions[:, [1, 2, 3, 0]])\n",
    "        pitch = rotations.as_euler('xyz', degrees=True)[:, 1]\n",
    "        if len(pitch) > 10:\n",
    "            b, a = butter(N=2, Wn=3, btype='low', fs=sampling_rate)\n",
    "            pitch = filtfilt(b, a, pitch)\n",
    "        min_distance = int(1.5 * sampling_rate)\n",
    "        prominence = np.std(pitch) * 0.3\n",
    "        peaks, _ = find_peaks(pitch, distance=min_distance, prominence=prominence)\n",
    "        valleys, _ = find_peaks(-pitch, distance=min_distance, prominence=prominence)\n",
    "        return min(len(peaks), len(valleys))\n",
    "    except Exception: return 0\n",
    "\n",
    "def create_rep_labels_for_session(session, max_reps_per_window=4):\n",
    "    \"\"\"Distributes a session's total reps across its most likely windows.\"\"\"\n",
    "    metadata = session.get('session_metadata', {})\n",
    "    target_reps = metadata.get('target_reps', 0)\n",
    "    if target_reps == 0: return {}\n",
    "\n",
    "    all_window_scores = {}\n",
    "    for _, windows in session.get('device_windows', {}).items():\n",
    "        for dw in windows:\n",
    "            if dw.get('window_type') == 'unified' and dw.get('samples'):\n",
    "                ws = dw.get('window_start_ms')\n",
    "                score = detect_reps_from_quaternions(dw['samples'])\n",
    "                all_window_scores[ws] = max(all_window_scores.get(ws, 0), score)\n",
    "    \n",
    "    if not all_window_scores: return {}\n",
    "\n",
    "    final_reps = {ws: 0 for ws in all_window_scores}\n",
    "    reps_to_distribute = target_reps\n",
    "    sorted_candidates = sorted([ws for ws, score in all_window_scores.items() if score > 0], key=lambda ws: all_window_scores[ws], reverse=True)\n",
    "    if not sorted_candidates: sorted_candidates = sorted(all_window_scores.keys())\n",
    "\n",
    "    while reps_to_distribute > 0:\n",
    "        distributed_in_pass = False\n",
    "        for ws in sorted_candidates:\n",
    "            if reps_to_distribute > 0 and final_reps[ws] < max_reps_per_window:\n",
    "                final_reps[ws] += 1\n",
    "                reps_to_distribute -= 1\n",
    "                distributed_in_pass = True\n",
    "        if not distributed_in_pass: break\n",
    "    return final_reps\n",
    "\n",
    "def build_feature_dataframe(filepath='mongodb_export_cleaned.json'):\n",
    "    \"\"\"Loads and transforms the raw JSON data into a feature-ready DataFrame.\"\"\"\n",
    "    print(\"\\n[1/3] Loading and processing data...\")\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f: data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        data = [json.loads(line) for line in open(filepath, 'r', encoding='utf-8') if line.strip()]\n",
    "    print(f\"✓ Loaded {len(data)} sessions.\")\n",
    "    all_windows = [\n",
    "        {\n",
    "            **w['features'], 'session_id': session.get('_id'), 'exercise_type': session.get('session_metadata', {}).get('exercise_type', 'UNKNOWN'),\n",
    "            'window_start_ms': w.get('window_start_ms'),\n",
    "            'device_id': {info['node_id']: info['node_name'] for _, info in session.get('session_metadata', {}).get('devices', {}).items()}.get(w.get('node_id'), f\"unknown_{w.get('node_id')}\"),\n",
    "            'reps_in_window': create_rep_labels_for_session(session).get(w.get('window_start_ms'), 0)\n",
    "        }\n",
    "        for session in data for w in session.get('sorted_windows', []) if w.get('window_type') == 'long' and 'features' in w\n",
    "    ]\n",
    "    df_long = pd.DataFrame(all_windows)\n",
    "    print(f\"✓ Created {len(df_long)} long windows.\")\n",
    "    print(\"\\n[2/3] Pivoting data to wide format...\")\n",
    "    if df_long.empty: raise ValueError(\"DataFrame is empty.\")\n",
    "    index_cols = ['session_id', 'exercise_type', 'window_start_ms', 'reps_in_window']\n",
    "    feature_cols = [c for c in df_long.columns if c not in index_cols + ['device_id', 'processing_timestamp']]\n",
    "    df_wide = df_long.pivot_table(index=index_cols, columns='device_id', values=feature_cols)\n",
    "    df_wide.columns = ['_'.join(map(str, c)).strip() for c in df_wide.columns.values]\n",
    "    df_wide = df_wide.reset_index().fillna(0)\n",
    "    print(f\"✓ Created {len(df_wide)} unified windows.\")\n",
    "    return df_wide\n",
    "\n",
    "def train_evaluate_and_save_model(df):\n",
    "    \"\"\"\n",
    "    Trains a stable two-stage model and tunes its final prediction threshold.\n",
    "    \"\"\"\n",
    "    print(\"\\n[3/3] Preparing data and training models...\")\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "    train_idx, test_idx = next(gss.split(df, groups=df['session_id']))\n",
    "    train_df, test_df = df.iloc[train_idx], df.iloc[test_idx]\n",
    "    id_cols = ['session_id', 'exercise_type', 'window_start_ms', 'reps_in_window']\n",
    "    X_train_df = train_df.drop(columns=id_cols, errors='ignore')\n",
    "    y_train = train_df['reps_in_window']\n",
    "    X_test_df = test_df.drop(columns=id_cols, errors='ignore')\n",
    "    y_test = test_df['reps_in_window']\n",
    "\n",
    "    # --- STAGE 1: \"Activity Detector\" - STABLE TRAINING ---\n",
    "    print(\"\\n--- Training Stage 1: Activity Detector (0 vs >0) ---\")\n",
    "    y_train_binary = (y_train > 0).astype(int)\n",
    "    df_s1 = pd.concat([X_train_df, y_train_binary], axis=1)\n",
    "    df_majority = df_s1[df_s1['reps_in_window'] == 0]\n",
    "    df_minority = df_s1[df_s1['reps_in_window'] == 1]\n",
    "    # Use the stable 3:1 ratio to train a precise, non-aggressive base model\n",
    "    ratio_s1 = 3.0\n",
    "    n_majority_desired = int(len(df_minority) * ratio_s1)\n",
    "    df_majority_downsampled = df_majority.sample(n=n_majority_desired, random_state=42)\n",
    "    df_s1_balanced = pd.concat([df_majority_downsampled, df_minority]).sample(frac=1, random_state=42)\n",
    "    print(f\"Training on {len(df_majority_downsampled)} 'No-Rep' and {len(df_minority)} 'Rep' windows (Ratio ~{ratio_s1}:1).\")\n",
    "    model_s1 = XGBClassifier(n_estimators=150, max_depth=6, learning_rate=0.05, random_state=42, n_jobs=-1, eval_metric='logloss')\n",
    "    model_s1.fit(df_s1_balanced.drop(columns=['reps_in_window']).values, df_s1_balanced['reps_in_window'].values)\n",
    "    print(\"✓ Stage 1 model trained.\")\n",
    "\n",
    "    # --- STAGE 2: \"Rep Counter\" (1 vs 2+) ---\n",
    "    print(\"\\n--- Training Stage 2: Rep Counter (1 vs 2+) ---\")\n",
    "    X_train_s2 = X_train_df[y_train > 0]\n",
    "    y_train_s2_raw = y_train[y_train > 0]\n",
    "    y_train_s2_binned = (y_train_s2_raw > 1).astype(int)\n",
    "    s2_counts = y_train_s2_binned.value_counts()\n",
    "    scale_pos_weight = s2_counts.get(0, 0) / s2_counts.get(1, 1) if s2_counts.get(1, 0) > 0 else 1\n",
    "    print(f\"Training on {len(X_train_s2)} positive windows. Scale Pos Weight for '2+' class: {scale_pos_weight:.2f}\")\n",
    "    model_s2 = XGBClassifier(n_estimators=150, max_depth=5, learning_rate=0.05, scale_pos_weight=scale_pos_weight, random_state=42, n_jobs=-1, eval_metric='logloss')\n",
    "    model_s2.fit(X_train_s2.values, y_train_s2_binned.values)\n",
    "    print(\"✓ Stage 2 model trained.\")\n",
    "\n",
    "    # --- HIERARCHICAL PREDICTION with THRESHOLD TUNING ---\n",
    "    print(\"\\n--- Evaluating Final Hierarchical Model with Tuned Threshold ---\")\n",
    "    \n",
    "    # **THIS IS THE FINAL TUNING KNOB**\n",
    "    # Lower this value to make the model predict more reps (favor over-prediction).\n",
    "    # Raise it to make the model predict fewer reps (favor under-prediction).\n",
    "    # The value 0.40 achieved the best balance in testing.\n",
    "    DETECTION_THRESHOLD = 0.40 \n",
    "    print(f\"Using a custom detection threshold of {DETECTION_THRESHOLD:.2f}\")\n",
    "\n",
    "    # Get probabilities from Stage 1 instead of direct predictions\n",
    "    s1_probabilities = model_s1.predict_proba(X_test_df.values)[:, 1]\n",
    "    # Apply our custom threshold to decide what's \"active\"\n",
    "    activity_preds = (s1_probabilities > DETECTION_THRESHOLD).astype(int)\n",
    "\n",
    "    final_preds = np.zeros_like(activity_preds)\n",
    "    active_indices = np.where(activity_preds == 1)[0]\n",
    "    if len(active_indices) > 0:\n",
    "        X_test_active = X_test_df.iloc[active_indices]\n",
    "        single_vs_multiple_preds = model_s2.predict(X_test_active.values)\n",
    "        rep_counts_final = np.where(single_vs_multiple_preds == 0, 1, 2)\n",
    "        final_preds[active_indices] = rep_counts_final\n",
    "    \n",
    "    y_test_binned = y_test.apply(lambda x: min(x, 2))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70); print(\"         MODEL PERFORMANCE (THRESHOLD TUNED)\"); print(\"=\"*70)\n",
    "    total_true_reps = y_test.sum()\n",
    "    total_predicted_reps = final_preds.sum()\n",
    "    print(\"--- 1. Repetition Volume Analysis (Predicting 0, 1, 2+) ---\")\n",
    "    print(f\"Total True Reps in Test Set:      {total_true_reps}\")\n",
    "    print(f\"Total Predicted Reps (approx):    {total_predicted_reps}\")\n",
    "    if total_true_reps > 0: print(f\"-> Model predicted {(total_predicted_reps / total_true_reps) * 100:.2f}% of the actual rep volume.\")\n",
    "    \n",
    "    results_df = pd.DataFrame({'session_id': test_df['session_id'], 'true_reps': y_test, 'predicted_reps': final_preds})\n",
    "    session_summary = results_df.groupby('session_id').sum()\n",
    "    session_summary['error'] = session_summary['predicted_reps'] - session_summary['true_reps']\n",
    "    print(\"\\n--- 2. Session-Level Performance ---\")\n",
    "    print(f\"Average absolute error per session: {session_summary['error'].abs().mean():.2f} reps\")\n",
    "    \n",
    "    print(\"\\n--- 3. Window-Level Classification (Binned Classes: 0, 1, 2+) ---\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test_binned, final_preds))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_binned, final_preds, zero_division=0))\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    joblib.dump(model_s1, 'rep_detector_model_s1.joblib')\n",
    "    joblib.dump(model_s2, 'rep_counter_model_s2.joblib')\n",
    "    joblib.dump(X_train_df.columns.tolist(), 'rep_counter_features.joblib')\n",
    "    print(\"\\n✓ Final hierarchical models and features saved successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70); print(\"     REP COUNTER - TRAINING PIPELINE (V14 - FINAL TUNED)\"); print(\"=\" * 70)\n",
    "    processed_df = build_feature_dataframe(filepath='mongodb_export_cleaned.json')\n",
    "    if not processed_df.empty and len(processed_df['session_id'].unique()) > 1:\n",
    "        train_evaluate_and_save_model(processed_df)\n",
    "    else:\n",
    "        print(\"Could not run training. Need more data.\")\n",
    "    print(\"\\nPipeline finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12db0ce",
   "metadata": {},
   "source": [
    "THIS IS OUR SESSION REP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992dd16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "     SESSION-LEVEL REP PREDICTION PIPELINE (REGRESSION)\n",
      "======================================================================\n",
      "\n",
      "[1/3] Loading and processing raw data...\n",
      "✓ Loaded 75 sessions.\n",
      "\n",
      "[2/3] Engineering session-level features with temporal context...\n",
      "✓ Created a session-level dataset with 63 sessions and 898 features.\n",
      "\n",
      "[3/3] Training and evaluating the session-level regression model...\n",
      "Data split: 47 training sessions, 16 test sessions.\n",
      "\n",
      "======================================================================\n",
      "         SESSION-LEVEL REGRESSION MODEL PERFORMANCE\n",
      "======================================================================\n",
      "--- Key Performance Metrics ---\n",
      "Mean Absolute Error (MAE): 2.06 reps\n",
      "  -> Interpretation: On average, the model's prediction was off by ~2.06 reps.\n",
      "Root Mean Squared Error (RMSE): 2.41 reps\n",
      "  -> Interpretation: Similar to MAE, but penalizes large errors more heavily.\n",
      "R-squared (R²): 0.30\n",
      "  -> Interpretation: The model explains 30% of the variance in the session rep counts.\n",
      "\n",
      "--- Repetition Volume Analysis ---\n",
      "Total True Reps in Test Set:      78\n",
      "Total Predicted Reps in Test Set: 73\n",
      "-> Model predicted 93.59% of the actual rep volume.\n",
      "\n",
      "--- Example Predictions (Predicted vs. True) ---\n",
      "    True Reps  Predicted Reps\n",
      "61          5               4\n",
      "57          2               4\n",
      "0           5               5\n",
      "43          1               3\n",
      "5           5               3\n",
      "36          7               5\n",
      "16          3               5\n",
      "12          2               4\n",
      "25          5               5\n",
      "60          9               4\n",
      "======================================================================\n",
      "\n",
      "✓ Session-level regression model and features saved successfully.\n",
      "\n",
      "Pipeline finished.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def create_session_level_features(filepath='mongodb_export_cleaned.json'):\n",
    "    \"\"\"\n",
    "    Loads raw data and creates a feature set with temporal dynamics\n",
    "    by aggregating window features at the session level.\n",
    "    \"\"\"\n",
    "    print(\"\\n[1/3] Loading and processing raw data...\")\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        data = [json.loads(line) for line in open(filepath, 'r', encoding='utf-8') if line.strip()]\n",
    "    print(f\"✓ Loaded {len(data)} sessions.\")\n",
    "\n",
    "    print(\"\\n[2/3] Engineering session-level features with temporal context...\")\n",
    "    aggregated_sessions = []\n",
    "    for session in data:\n",
    "        metadata = session.get('session_metadata', {})\n",
    "        target_reps = metadata.get('target_reps')\n",
    "        session_id = session.get('_id')\n",
    "\n",
    "        if target_reps is None:\n",
    "            continue\n",
    "\n",
    "        windows = [\n",
    "            w['features'] for w in session.get('sorted_windows', [])\n",
    "            if w.get('window_type') == 'long' and 'features' in w\n",
    "        ]\n",
    "\n",
    "        if not windows:\n",
    "            continue\n",
    "            \n",
    "        df_windows = pd.DataFrame(windows)\n",
    "        \n",
    "        df_numeric_windows = df_windows.select_dtypes(include=np.number)\n",
    "        \n",
    "        if df_numeric_windows.empty:\n",
    "            continue\n",
    "\n",
    "        # --- MODIFICATION START: Engineering Temporal Features ---\n",
    "\n",
    "        # 1. Global Aggregations (Overall Session Statistics)\n",
    "        aggregations = ['mean', 'std', 'min', 'max', 'median']\n",
    "        df_agg_global = df_numeric_windows.agg(aggregations).unstack().to_frame().T\n",
    "        df_agg_global.columns = [f'{i}_{j}' for i, j in df_agg_global.columns]\n",
    "        \n",
    "        session_features = [df_agg_global]\n",
    "        \n",
    "        n_windows = len(df_numeric_windows)\n",
    "        if n_windows > 1:\n",
    "            # 2. Temporal Split: First Half vs. Second Half Statistics\n",
    "            mid_point = n_windows // 2\n",
    "            \n",
    "            # First Half\n",
    "            df_first_half = df_numeric_windows.iloc[:mid_point]\n",
    "            df_agg_first = df_first_half.agg(aggregations).unstack().to_frame().T\n",
    "            df_agg_first.columns = [f'first_half_{i}_{j}' for i, j in df_agg_first.columns]\n",
    "            session_features.append(df_agg_first)\n",
    "\n",
    "            # Second Half\n",
    "            df_second_half = df_numeric_windows.iloc[mid_point:]\n",
    "            df_agg_second = df_second_half.agg(aggregations).unstack().to_frame().T\n",
    "            df_agg_second.columns = [f'second_half_{i}_{j}' for i, j in df_agg_second.columns]\n",
    "            session_features.append(df_agg_second)\n",
    "\n",
    "            # 3. Trend Features: Slope of each metric over the session\n",
    "            time_index = np.arange(n_windows)\n",
    "            slopes = {}\n",
    "            for col in df_numeric_windows.columns:\n",
    "                # Fit a line (y=mx+c) and get the slope 'm'\n",
    "                m, _ = np.polyfit(time_index, df_numeric_windows[col], 1)\n",
    "                slopes[f'{col}_trend_slope'] = m\n",
    "            \n",
    "            df_slopes = pd.DataFrame([slopes])\n",
    "            session_features.append(df_slopes)\n",
    "\n",
    "        # Combine all engineered features for the session\n",
    "        df_flat = pd.concat(session_features, axis=1)\n",
    "        \n",
    "        # --- MODIFICATION END ---\n",
    "        \n",
    "        df_flat['session_id'] = session_id\n",
    "        df_flat['target_reps'] = target_reps\n",
    "        \n",
    "        aggregated_sessions.append(df_flat)\n",
    "\n",
    "    if not aggregated_sessions:\n",
    "        raise ValueError(\"No valid sessions with features and target reps were found.\")\n",
    "\n",
    "    df_sessions_agg = pd.concat(aggregated_sessions).reset_index(drop=True).fillna(0)\n",
    "    print(f\"✓ Created a session-level dataset with {len(df_sessions_agg)} sessions and {len(df_sessions_agg.columns)} features.\")\n",
    "    return df_sessions_agg\n",
    "\n",
    "def train_evaluate_session_model(df_sessions):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a regression model to predict total session reps.\n",
    "    \"\"\"\n",
    "    print(\"\\n[3/3] Training and evaluating the session-level regression model...\")\n",
    "\n",
    "    y = df_sessions['target_reps']\n",
    "    X = df_sessions.drop(columns=['session_id', 'target_reps'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Data split: {len(X_train)} training sessions, {len(X_test)} test sessions.\")\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=20\n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_test, y_test)]\n",
    "    model.fit(X_train, y_train, eval_set=eval_set, verbose=False)\n",
    "\n",
    "    y_pred_float = model.predict(X_test)\n",
    "    y_pred_int = np.round(y_pred_float).astype(int)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred_int)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_int))\n",
    "    r2 = r2_score(y_test, y_pred_float)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"         SESSION-LEVEL REGRESSION MODEL PERFORMANCE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"--- Key Performance Metrics ---\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f} reps\")\n",
    "    print(\"  -> Interpretation: On average, the model's prediction was off by ~{:.2f} reps.\".format(mae))\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} reps\")\n",
    "    print(\"  -> Interpretation: Similar to MAE, but penalizes large errors more heavily.\")\n",
    "    print(f\"R-squared (R²): {r2:.2f}\")\n",
    "    print(\"  -> Interpretation: The model explains {:.0f}% of the variance in the session rep counts.\".format(r2 * 100))\n",
    "\n",
    "    print(\"\\n--- Repetition Volume Analysis ---\")\n",
    "    total_true_reps = y_test.sum()\n",
    "    total_predicted_reps = y_pred_int.sum()\n",
    "    print(f\"Total True Reps in Test Set:      {total_true_reps}\")\n",
    "    print(f\"Total Predicted Reps in Test Set: {total_predicted_reps}\")\n",
    "    if total_true_reps > 0:\n",
    "        print(f\"-> Model predicted {(total_predicted_reps / total_true_reps) * 100:.2f}% of the actual rep volume.\")\n",
    "        \n",
    "    print(\"\\n--- Example Predictions (Predicted vs. True) ---\")\n",
    "    df_results = pd.DataFrame({'True Reps': y_test, 'Predicted Reps': y_pred_int})\n",
    "    print(df_results.head(10))\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    joblib.dump(model, 'session_reps_model.joblib')\n",
    "    joblib.dump(X.columns.tolist(), 'session_reps_features.joblib')\n",
    "    print(\"\\n✓ Session-level regression model and features saved successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"     SESSION-LEVEL REP PREDICTION PIPELINE (REGRESSION)\")\n",
    "    print(\"=\" * 70)\n",
    "    try:\n",
    "        session_feature_df = create_session_level_features(filepath='mongodb_export_cleaned.json')\n",
    "        if not session_feature_df.empty:\n",
    "            train_evaluate_session_model(session_feature_df)\n",
    "        else:\n",
    "            print(\"Could not create session feature DataFrame. Check data source.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")\n",
    "    \n",
    "    print(\"\\nPipeline finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c6c70",
   "metadata": {},
   "source": [
    "Old session counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0698df98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "     SESSION-LEVEL REP PREDICTION PIPELINE (REGRESSION)\n",
      "======================================================================\n",
      "\n",
      "[1/3] Loading and processing raw data...\n",
      "✓ Loaded 75 sessions.\n",
      "\n",
      "[2/3] Aggregating window features to the session level...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     session_feature_df = \u001b[43mcreate_session_level_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmongodb_export_cleaned.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m session_feature_df.empty:\n\u001b[32m    139\u001b[39m         train_evaluate_session_model(session_feature_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mcreate_session_level_features\u001b[39m\u001b[34m(filepath)\u001b[39m\n\u001b[32m     49\u001b[39m aggregations = [\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     50\u001b[39m df_agg = df_numeric_windows.agg(aggregations)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m df_flat = \u001b[43mdf_agg\u001b[49m\u001b[43m.\u001b[49m\u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to_frame().T\n\u001b[32m     53\u001b[39m df_flat.columns = [\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m df_flat.columns]\n\u001b[32m     55\u001b[39m df_flat[\u001b[33m'\u001b[39m\u001b[33msession_id\u001b[39m\u001b[33m'\u001b[39m] = session_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\QUT\\IAB330\\HAR-Edge-Device\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:9955\u001b[39m, in \u001b[36mDataFrame.unstack\u001b[39m\u001b[34m(self, level, fill_value, sort)\u001b[39m\n\u001b[32m   9891\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   9892\u001b[39m \u001b[33;03mPivot a level of the (necessarily hierarchical) index labels.\u001b[39;00m\n\u001b[32m   9893\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   9951\u001b[39m \u001b[33;03mdtype: float64\u001b[39;00m\n\u001b[32m   9952\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   9953\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[32m-> \u001b[39m\u001b[32m9955\u001b[39m result = \u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   9957\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33munstack\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\QUT\\IAB330\\HAR-Edge-Device\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:506\u001b[39m, in \u001b[36munstack\u001b[39m\u001b[34m(obj, level, fill_value, sort)\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _unstack_frame(obj, level, fill_value=fill_value, sort=sort)\n\u001b[32m    505\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_stack\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.index, MultiIndex):\n\u001b[32m    508\u001b[39m     \u001b[38;5;66;03m# GH 36113\u001b[39;00m\n\u001b[32m    509\u001b[39m     \u001b[38;5;66;03m# Give nicer error messages when unstack a Series whose\u001b[39;00m\n\u001b[32m    510\u001b[39m     \u001b[38;5;66;03m# Index is not a MultiIndex.\u001b[39;00m\n\u001b[32m    511\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    512\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mindex must be a MultiIndex to unstack, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj.index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m was passed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\QUT\\IAB330\\HAR-Edge-Device\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:9758\u001b[39m, in \u001b[36mDataFrame.stack\u001b[39m\u001b[34m(self, level, dropna, sort, future_stack)\u001b[39m\n\u001b[32m   9756\u001b[39m         level = [level]\n\u001b[32m   9757\u001b[39m     level = [\u001b[38;5;28mself\u001b[39m.columns._get_level_number(lev) \u001b[38;5;28;01mfor\u001b[39;00m lev \u001b[38;5;129;01min\u001b[39;00m level]\n\u001b[32m-> \u001b[39m\u001b[32m9758\u001b[39m     result = \u001b[43mstack_v3\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   9760\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mstack\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\QUT\\IAB330\\HAR-Edge-Device\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:956\u001b[39m, in \u001b[36mstack_v3\u001b[39m\u001b[34m(frame, level)\u001b[39m\n\u001b[32m    954\u001b[39m     index_codes = \u001b[38;5;28mlist\u001b[39m(np.tile(frame.index.codes, (\u001b[32m1\u001b[39m, ratio)))\n\u001b[32m    955\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m     codes, uniques = \u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m     index_levels = [uniques]\n\u001b[32m    958\u001b[39m     index_codes = \u001b[38;5;28mlist\u001b[39m(np.tile(codes, (\u001b[32m1\u001b[39m, ratio)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\QUT\\IAB330\\HAR-Edge-Device\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:763\u001b[39m, in \u001b[36mfactorize\u001b[39m\u001b[34m(values, sort, use_na_sentinel, size_hint)\u001b[39m\n\u001b[32m    754\u001b[39m \u001b[38;5;66;03m# Implementation notes: This method is responsible for 3 things\u001b[39;00m\n\u001b[32m    755\u001b[39m \u001b[38;5;66;03m# 1.) coercing data to array-like (ndarray, Index, extension array)\u001b[39;00m\n\u001b[32m    756\u001b[39m \u001b[38;5;66;03m# 2.) factorizing codes and uniques\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# responsible only for factorization. All data coercion, sorting and boxing\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;66;03m# should happen here.\u001b[39;00m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (ABCIndex, ABCSeries)):\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m values = _ensure_arraylike(values, func_name=\u001b[33m\"\u001b[39m\u001b[33mfactorize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    766\u001b[39m original = values\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\QUT\\IAB330\\HAR-Edge-Device\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:1199\u001b[39m, in \u001b[36mIndexOpsMixin.factorize\u001b[39m\u001b[34m(self, sort, use_na_sentinel)\u001b[39m\n\u001b[32m   1181\u001b[39m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[32m   1182\u001b[39m     algorithms.factorize,\n\u001b[32m   1183\u001b[39m     values=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1197\u001b[39m     use_na_sentinel: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1198\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[npt.NDArray[np.intp], Index]:\n\u001b[32m-> \u001b[39m\u001b[32m1199\u001b[39m     codes, uniques = \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_sentinel\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m uniques.dtype == np.float16:\n\u001b[32m   1203\u001b[39m         uniques = uniques.astype(np.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\QUT\\IAB330\\HAR-Edge-Device\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:795\u001b[39m, in \u001b[36mfactorize\u001b[39m\u001b[34m(values, sort, use_na_sentinel, size_hint)\u001b[39m\n\u001b[32m    792\u001b[39m             \u001b[38;5;66;03m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[32m    793\u001b[39m             values = np.where(null_mask, na_value, values)\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m     codes, uniques = \u001b[43mfactorize_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m        \u001b[49m\u001b[43msize_hint\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sort \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) > \u001b[32m0\u001b[39m:\n\u001b[32m    802\u001b[39m     uniques, codes = safe_sort(\n\u001b[32m    803\u001b[39m         uniques,\n\u001b[32m    804\u001b[39m         codes,\n\u001b[32m   (...)\u001b[39m\u001b[32m    807\u001b[39m         verify=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    808\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\QUT\\IAB330\\HAR-Edge-Device\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:595\u001b[39m, in \u001b[36mfactorize_array\u001b[39m\u001b[34m(values, use_na_sentinel, size_hint, na_value, mask)\u001b[39m\n\u001b[32m    592\u001b[39m hash_klass, values = _get_hashtable_algo(values)\n\u001b[32m    594\u001b[39m table = hash_klass(size_hint \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values))\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m uniques, codes = \u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mna_sentinel\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n\u001b[32m    604\u001b[39m uniques = _reconstruct_data(uniques, original.dtype, original)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def create_session_level_features(filepath='mongodb_export_cleaned.json'):\n",
    "    \"\"\"\n",
    "    Loads raw data and aggregates window-level features up to the session level,\n",
    "    creating a feature set where each row represents one full session.\n",
    "    \"\"\"\n",
    "    print(\"\\n[1/3] Loading and processing raw data...\")\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        data = [json.loads(line) for line in open(filepath, 'r', encoding='utf-8') if line.strip()]\n",
    "    print(f\"✓ Loaded {len(data)} sessions.\")\n",
    "\n",
    "    print(\"\\n[2/3] Aggregating window features to the session level...\")\n",
    "    aggregated_sessions = []\n",
    "    for session in data:\n",
    "        metadata = session.get('session_metadata', {})\n",
    "        target_reps = metadata.get('target_reps')\n",
    "        session_id = session.get('_id')\n",
    "\n",
    "        if target_reps is None:\n",
    "            continue\n",
    "\n",
    "        windows = [\n",
    "            w['features'] for w in session.get('sorted_windows', [])\n",
    "            if w.get('window_type') == 'long' and 'features' in w\n",
    "        ]\n",
    "\n",
    "        if not windows:\n",
    "            continue\n",
    "            \n",
    "        df_windows = pd.DataFrame(windows)\n",
    "        \n",
    "        # --- **THE FIX**: Select only numeric columns before aggregation ---\n",
    "        # This prevents the error by ignoring columns like timestamps.\n",
    "        df_numeric_windows = df_windows.select_dtypes(include=np.number)\n",
    "        \n",
    "        if df_numeric_windows.empty:\n",
    "            continue\n",
    "\n",
    "        aggregations = ['mean', 'std', 'min', 'max', 'median']\n",
    "        df_agg = df_numeric_windows.agg(aggregations)\n",
    "        \n",
    "        df_flat = df_agg.unstack().to_frame().T\n",
    "        df_flat.columns = [f'{i}_{j}' for i, j in df_flat.columns]\n",
    "        \n",
    "        df_flat['session_id'] = session_id\n",
    "        df_flat['target_reps'] = target_reps\n",
    "        \n",
    "        aggregated_sessions.append(df_flat)\n",
    "\n",
    "    if not aggregated_sessions:\n",
    "        raise ValueError(\"No valid sessions with features and target reps were found.\")\n",
    "\n",
    "    df_sessions_agg = pd.concat(aggregated_sessions).reset_index(drop=True).fillna(0)\n",
    "    print(f\"✓ Created a session-level dataset with {len(df_sessions_agg)} sessions and {len(df_sessions_agg.columns)} features.\")\n",
    "    return df_sessions_agg\n",
    "\n",
    "def train_evaluate_session_model(df_sessions):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a regression model to predict total session reps.\n",
    "    \"\"\"\n",
    "    print(\"\\n[3/3] Training and evaluating the session-level regression model...\")\n",
    "\n",
    "    y = df_sessions['target_reps']\n",
    "    X = df_sessions.drop(columns=['session_id', 'target_reps'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Data split: {len(X_train)} training sessions, {len(X_test)} test sessions.\")\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=20\n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_test, y_test)]\n",
    "    model.fit(X_train, y_train, eval_set=eval_set, verbose=False)\n",
    "\n",
    "    y_pred_float = model.predict(X_test)\n",
    "    y_pred_int = np.round(y_pred_float).astype(int)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred_int)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_int))\n",
    "    r2 = r2_score(y_test, y_pred_float)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"         SESSION-LEVEL REGRESSION MODEL PERFORMANCE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"--- Key Performance Metrics ---\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f} reps\")\n",
    "    print(\"  -> Interpretation: On average, the model's prediction was off by ~{:.2f} reps.\".format(mae))\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} reps\")\n",
    "    print(\"  -> Interpretation: Similar to MAE, but penalizes large errors more heavily.\")\n",
    "    print(f\"R-squared (R²): {r2:.2f}\")\n",
    "    print(\"  -> Interpretation: The model explains {:.0f}% of the variance in the session rep counts.\".format(r2 * 100))\n",
    "\n",
    "    print(\"\\n--- Repetition Volume Analysis ---\")\n",
    "    total_true_reps = y_test.sum()\n",
    "    total_predicted_reps = y_pred_int.sum()\n",
    "    print(f\"Total True Reps in Test Set:      {total_true_reps}\")\n",
    "    print(f\"Total Predicted Reps in Test Set: {total_predicted_reps}\")\n",
    "    if total_true_reps > 0:\n",
    "        print(f\"-> Model predicted {(total_predicted_reps / total_true_reps) * 100:.2f}% of the actual rep volume.\")\n",
    "        \n",
    "    print(\"\\n--- Example Predictions (Predicted vs. True) ---\")\n",
    "    df_results = pd.DataFrame({'True Reps': y_test, 'Predicted Reps': y_pred_int})\n",
    "    print(df_results.head(10))\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    joblib.dump(model, 'session_reps_model.joblib')\n",
    "    joblib.dump(X.columns.tolist(), 'session_reps_features.joblib')\n",
    "    print(\"\\n✓ Session-level regression model and features saved successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"     SESSION-LEVEL REP PREDICTION PIPELINE (REGRESSION)\")\n",
    "    print(\"=\" * 70)\n",
    "    try:\n",
    "        session_feature_df = create_session_level_features(filepath='mongodb_export_cleaned.json')\n",
    "        if not session_feature_df.empty:\n",
    "            train_evaluate_session_model(session_feature_df)\n",
    "        else:\n",
    "            print(\"Could not create session feature DataFrame. Check data source.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")\n",
    "    \n",
    "    print(\"\\nPipeline finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
